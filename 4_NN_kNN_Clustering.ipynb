{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTzwRLy0CJwG"
      },
      "source": [
        "# Data Loading and Preprocessing\n",
        "\n",
        "We consider the same notebook used in the labs, containing house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n",
        "\n",
        "https://www.kaggle.com/harlfoxem/housesalesprediction\n",
        "\n",
        "For each house we know 18 house features (e.g., number of bedrooms, number of bathrooms, etc.) plus its price, that is what we would like to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZvcPd62CJwN"
      },
      "source": [
        "## TO DO: Insert your ID number (\"numero di matricola\") below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6Q2cJpWCJwO"
      },
      "outputs": [],
      "source": [
        "#put here your ``numero di matricola''\n",
        "numero_di_matricola = 2019157"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS6fkT2PCJwP"
      },
      "source": [
        "Load the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU1pYZ3JCJwQ"
      },
      "outputs": [],
      "source": [
        "#import all packages needed\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLTXwJPECJwR"
      },
      "source": [
        "Read the data, remove data samples/points with missing values (NaN), and print some statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAWd62ASCJwR",
        "outputId": "cf74d768-ca4f-4ee4-c2bf-36de53668bd2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>price</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3.164000e+03</td>\n",
              "      <td>3.164000e+03</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3.164000e+03</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "      <td>3164.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.645240e+09</td>\n",
              "      <td>5.354358e+05</td>\n",
              "      <td>3.381163</td>\n",
              "      <td>2.071903</td>\n",
              "      <td>2070.027813</td>\n",
              "      <td>1.525054e+04</td>\n",
              "      <td>1.434893</td>\n",
              "      <td>0.009798</td>\n",
              "      <td>0.244311</td>\n",
              "      <td>3.459229</td>\n",
              "      <td>7.615676</td>\n",
              "      <td>1761.252212</td>\n",
              "      <td>308.775601</td>\n",
              "      <td>1967.489254</td>\n",
              "      <td>94.668774</td>\n",
              "      <td>98077.125158</td>\n",
              "      <td>47.557868</td>\n",
              "      <td>-122.212337</td>\n",
              "      <td>1982.544564</td>\n",
              "      <td>13176.302465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.854203e+09</td>\n",
              "      <td>3.809004e+05</td>\n",
              "      <td>0.895472</td>\n",
              "      <td>0.768212</td>\n",
              "      <td>920.251879</td>\n",
              "      <td>4.254457e+04</td>\n",
              "      <td>0.507792</td>\n",
              "      <td>0.098513</td>\n",
              "      <td>0.776298</td>\n",
              "      <td>0.682592</td>\n",
              "      <td>1.166324</td>\n",
              "      <td>815.934864</td>\n",
              "      <td>458.977904</td>\n",
              "      <td>28.095275</td>\n",
              "      <td>424.439427</td>\n",
              "      <td>54.172937</td>\n",
              "      <td>0.140789</td>\n",
              "      <td>0.139577</td>\n",
              "      <td>686.256670</td>\n",
              "      <td>25413.180755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000102e+06</td>\n",
              "      <td>7.500000e+04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>380.000000</td>\n",
              "      <td>6.490000e+02</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>380.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1900.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>98001.000000</td>\n",
              "      <td>47.177500</td>\n",
              "      <td>-122.514000</td>\n",
              "      <td>620.000000</td>\n",
              "      <td>660.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.199775e+09</td>\n",
              "      <td>3.150000e+05</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1430.000000</td>\n",
              "      <td>5.453750e+03</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1950.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>98032.000000</td>\n",
              "      <td>47.459575</td>\n",
              "      <td>-122.324250</td>\n",
              "      <td>1480.000000</td>\n",
              "      <td>5429.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.027701e+09</td>\n",
              "      <td>4.450000e+05</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1910.000000</td>\n",
              "      <td>8.000000e+03</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1545.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1969.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>98059.000000</td>\n",
              "      <td>47.572500</td>\n",
              "      <td>-122.226000</td>\n",
              "      <td>1830.000000</td>\n",
              "      <td>7873.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.358175e+09</td>\n",
              "      <td>6.402500e+05</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>1.122250e+04</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2150.000000</td>\n",
              "      <td>600.000000</td>\n",
              "      <td>1990.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>98117.000000</td>\n",
              "      <td>47.680250</td>\n",
              "      <td>-122.124000</td>\n",
              "      <td>2360.000000</td>\n",
              "      <td>10408.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.839301e+09</td>\n",
              "      <td>5.350000e+06</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>8010.000000</td>\n",
              "      <td>1.651359e+06</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>6720.000000</td>\n",
              "      <td>2620.000000</td>\n",
              "      <td>2015.000000</td>\n",
              "      <td>2015.000000</td>\n",
              "      <td>98199.000000</td>\n",
              "      <td>47.777600</td>\n",
              "      <td>-121.315000</td>\n",
              "      <td>5790.000000</td>\n",
              "      <td>425581.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id         price     bedrooms    bathrooms  sqft_living  \\\n",
              "count  3.164000e+03  3.164000e+03  3164.000000  3164.000000  3164.000000   \n",
              "mean   4.645240e+09  5.354358e+05     3.381163     2.071903  2070.027813   \n",
              "std    2.854203e+09  3.809004e+05     0.895472     0.768212   920.251879   \n",
              "min    1.000102e+06  7.500000e+04     0.000000     0.000000   380.000000   \n",
              "25%    2.199775e+09  3.150000e+05     3.000000     1.500000  1430.000000   \n",
              "50%    4.027701e+09  4.450000e+05     3.000000     2.000000  1910.000000   \n",
              "75%    7.358175e+09  6.402500e+05     4.000000     2.500000  2500.000000   \n",
              "max    9.839301e+09  5.350000e+06     8.000000     6.000000  8010.000000   \n",
              "\n",
              "           sqft_lot       floors   waterfront         view    condition  \\\n",
              "count  3.164000e+03  3164.000000  3164.000000  3164.000000  3164.000000   \n",
              "mean   1.525054e+04     1.434893     0.009798     0.244311     3.459229   \n",
              "std    4.254457e+04     0.507792     0.098513     0.776298     0.682592   \n",
              "min    6.490000e+02     1.000000     0.000000     0.000000     1.000000   \n",
              "25%    5.453750e+03     1.000000     0.000000     0.000000     3.000000   \n",
              "50%    8.000000e+03     1.000000     0.000000     0.000000     3.000000   \n",
              "75%    1.122250e+04     2.000000     0.000000     0.000000     4.000000   \n",
              "max    1.651359e+06     3.500000     1.000000     4.000000     5.000000   \n",
              "\n",
              "             grade   sqft_above  sqft_basement     yr_built  yr_renovated  \\\n",
              "count  3164.000000  3164.000000    3164.000000  3164.000000   3164.000000   \n",
              "mean      7.615676  1761.252212     308.775601  1967.489254     94.668774   \n",
              "std       1.166324   815.934864     458.977904    28.095275    424.439427   \n",
              "min       3.000000   380.000000       0.000000  1900.000000      0.000000   \n",
              "25%       7.000000  1190.000000       0.000000  1950.000000      0.000000   \n",
              "50%       7.000000  1545.000000       0.000000  1969.000000      0.000000   \n",
              "75%       8.000000  2150.000000     600.000000  1990.000000      0.000000   \n",
              "max      12.000000  6720.000000    2620.000000  2015.000000   2015.000000   \n",
              "\n",
              "            zipcode          lat         long  sqft_living15     sqft_lot15  \n",
              "count   3164.000000  3164.000000  3164.000000    3164.000000    3164.000000  \n",
              "mean   98077.125158    47.557868  -122.212337    1982.544564   13176.302465  \n",
              "std       54.172937     0.140789     0.139577     686.256670   25413.180755  \n",
              "min    98001.000000    47.177500  -122.514000     620.000000     660.000000  \n",
              "25%    98032.000000    47.459575  -122.324250    1480.000000    5429.500000  \n",
              "50%    98059.000000    47.572500  -122.226000    1830.000000    7873.000000  \n",
              "75%    98117.000000    47.680250  -122.124000    2360.000000   10408.250000  \n",
              "max    98199.000000    47.777600  -121.315000    5790.000000  425581.000000  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#load the data\n",
        "df = pd.read_csv('kc_house_data.csv', sep = ',')\n",
        "\n",
        "#remove the data samples with missing values (NaN)\n",
        "df = df.dropna() \n",
        "\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9imOEDQwCJwT"
      },
      "source": [
        "Get the feature matrix and the vector of target values. We want to predict the price by using features other than id as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhZhSiMOCJwT",
        "outputId": "edd4bef9-7200-4366-b3c3-e00088307696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of data: 3164\n"
          ]
        }
      ],
      "source": [
        "Data = df.values\n",
        "# m = number of input samples\n",
        "m = Data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "Y = Data[:m,2]\n",
        "X = Data[:m,3:]\n",
        "\n",
        "feature_names = df.columns[3:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNc7kmTXCJwU"
      },
      "source": [
        "We split the $m$ samples of the data into 3 parts: one will be used for training and choosing the parameters, one for choosing among different models, and one for testing. The part for training and choosing the parameters will consist of $m_{train}=2/3 m$ samples, the one for choosing among different models will consist of $m_{val}= (m - m_{train})/2$ samples, while the other part consists of $m_{test}=m - m_{train} - m_{val}$ samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BweGCaX_CJwV",
        "outputId": "aaf463b4-b06e-41b4-dd96-707f3eb081d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of data for training and deciding parameters: 2109\n",
            "Amount of data for validation (choosing among different models): 527\n",
            "Amount of data for test: 528\n"
          ]
        }
      ],
      "source": [
        "# Split data into train (2/3 of samples), validation (1/6 of samples), and test data (the rest)\n",
        "m_train = int(2./3.*m)\n",
        "m_val = int((m-m_train)/2.)\n",
        "m_test = m - m_train - m_val\n",
        "print(\"Amount of data for training and deciding parameters:\",m_train)\n",
        "print(\"Amount of data for validation (choosing among different models):\",m_val)\n",
        "print(\"Amount of data for test:\",m_test)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Xtrain_and_val, Ytrain_and_val is the part of data for training and validation\n",
        "#Xtest, Ytest is the part of data for testing\n",
        "Xtrain_and_val, Xtest, Ytrain_and_val, Ytest = train_test_split(X, Y, test_size=m_test/m, random_state=numero_di_matricola)\n",
        "\n",
        "#if you need to consider a specific training and validation split, use\n",
        "#Xtrain, Ytrain for training and Xval, Yval for validation\n",
        "Xtrain, Xval, Ytrain, Yval = train_test_split(Xtrain_and_val, Ytrain_and_val, test_size=m_val/(m_train+m_val), random_state=numero_di_matricola)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsoruwcxCJwW"
      },
      "source": [
        "Let's scale the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74ZBiZXGCJwW"
      },
      "outputs": [],
      "source": [
        "# Data pre-processing\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler().fit(Xtrain)\n",
        "Xtrain_scaled = scaler.transform(Xtrain)\n",
        "Xtrain_and_val_scaled = scaler.transform(Xtrain_and_val)\n",
        "Xval_scaled = scaler.transform(Xval)\n",
        "Xtest_scaled = scaler.transform(Xtest)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C87iesotCJwX"
      },
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t30o2_TPCJwY"
      },
      "source": [
        "Let's learn the best neural network with 1 hidden layer and between 1 and 9 hidden nodes, choosing the best number of hidden nodes with cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlVeIPm4CJwY",
        "outputId": "84e00a7b-d0b9-4176-e836-20979b416277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:   32.3s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=MLPRegressor(max_iter=2000),\n",
              "             param_grid={'activation': ['relu'],\n",
              "                         'hidden_layer_sizes': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
              "                         'random_state': [2019157], 'solver': ['lbfgs']},\n",
              "             verbose=True)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "mlp_cv = MLPRegressor(max_iter=2000)\n",
        "param_grid = {'hidden_layer_sizes': [i for i in range(1,10)],\n",
        "              'activation': ['relu'],\n",
        "              'solver': ['lbfgs'], \n",
        "              'random_state': [numero_di_matricola],\n",
        "             }\n",
        "mlp_GS = GridSearchCV( mlp_cv, param_grid=param_grid, cv=5, verbose=True)\n",
        "mlp_GS.fit(Xtrain_and_val_scaled, Ytrain_and_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nd-2eilCJwZ"
      },
      "source": [
        "Now let's check what is the best parameter, and compare the best NNs with the linear model (learned on train and validation) on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXhjekRSCJwZ",
        "outputId": "9ea22979-1cbe-41e0-8e91-35784f9c4e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model:  MLPRegressor(hidden_layer_sizes=5, max_iter=2000, random_state=2019157,\n",
            "             solver='lbfgs')\n",
            "Error (1-R^2) of best model:  0.1953286353963405\n"
          ]
        }
      ],
      "source": [
        "#let's print the best model according to grid search\n",
        "print(\"Best model: \",mlp_GS.best_estimator_)\n",
        "#let's print the error 1-R^2 for the best model\n",
        "print(\"Error (1-R^2) of best model: \",1. - mlp_GS.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KawXANFCCJwZ"
      },
      "source": [
        "Let's learn the best NN using all of training and validation, and then compare the error of the best NN on train and validation and on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYlL3aDUCJwa",
        "outputId": "d24c4be4-1400-402a-e93e-aed635b3cfb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error best model on train and validation:  0.14124494388261788\n",
            "Error best model on test data:  0.18499202712189133\n"
          ]
        }
      ],
      "source": [
        "#best_mlp = MLPRegressor(max_iter=2000, hidden_layer_sizes=(5,), activation='relu', solver='lbfgs', random_state = numero_di_matricola)\n",
        "best_mlp = mlp_GS.best_estimator_\n",
        "best_mlp.fit(Xtrain_and_val_scaled,Ytrain_and_val)\n",
        "\n",
        "print(\"Error best model on train and validation: \",1. - best_mlp.score(Xtrain_and_val_scaled,Ytrain_and_val))\n",
        "print(\"Error best model on test data: \",1. - best_mlp.score(Xtest_scaled,Ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCWGhrtFCJwa"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYXVdDgmCJwa"
      },
      "source": [
        "Now let's learn the linear model on train and validation, and get error (1-R^2) on train and validation and on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CltOVPJdCJwb",
        "outputId": "56ba7893-48bc-43a9-e368-e0f842ef49aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 - coefficient of determination on training data:0.27013072490104273\n",
            "1 - coefficient of determination on test data:0.326693824565925\n"
          ]
        }
      ],
      "source": [
        "from sklearn import linear_model\n",
        "#LR the linear regression model\n",
        "LR = linear_model.LinearRegression()\n",
        "\n",
        "#fit the model on training data\n",
        "LR.fit(Xtrain_and_val_scaled, Ytrain_and_val)\n",
        "\n",
        "print(\"1 - coefficient of determination on training data:\"+str(1 - LR.score(Xtrain_and_val_scaled,Ytrain_and_val)))\n",
        "print(\"1 - coefficient of determination on test data:\"+str(1 - LR.score(Xtest_scaled,Ytest)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGSiiofMCJwb"
      },
      "source": [
        "# k-Nearest Neighbours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXA5mZj-CJwc"
      },
      "source": [
        "You will now explore the k-Nearest Neighbours (kNN) method for regression. In order to do this, you will need to use load the scikit-learn package *neighbors.KNeighborsRegressor* \n",
        "\n",
        "k-Nearest Neighbours for regression works as follows: the predicted value $h(\\textbf{x})$ for an instance $\\textbf{x}$ is obtained by first finding the $\\ell$ instances *in the training set* that are clostest to $\\textbf{x}$; the predicted value $h(\\textbf{x})$ is then the mean of the targets of such $\\ell$ instances. $\\ell$ is a parameter of the method. The targets of the $\\ell$ instances used for prediction can be weighted by the (inverse of) their distance to $\\textbf{x}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWXO9xMMCJwc"
      },
      "source": [
        "## TO DO: load the package for kNN regression, learn the model with default parameters using the training and validation scaled data, and print the error (1-R^2) on the data used to train the model and on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OALJRVgTCJwc",
        "outputId": "4b451cbc-4f90-4fa6-d340-29042d4e6dcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error on train and validation:  0.1397947148596227\n",
            "Error on test data:  0.31539170780529113\n"
          ]
        }
      ],
      "source": [
        "#import package\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "neigh = KNeighborsRegressor()\n",
        "#TO DO: learn model\n",
        "#fit the model on training and validation scaled data\n",
        "neigh.fit(Xtrain_and_val_scaled, Ytrain_and_val)\n",
        "\n",
        "\n",
        "print(\"Error on train and validation: \",1. - neigh.score(Xtrain_and_val_scaled,Ytrain_and_val))\n",
        "print(\"Error on test data: \",1. - neigh.score(Xtest_scaled,Ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF5v0LtrCJwc"
      },
      "source": [
        "## TO DO: repeat the point (including the printing instructions) above using the kNN version where points are weighted by the inverse of their distance "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg90aTuQCJwd",
        "outputId": "de3a9ec8-11ac-489a-b38f-9d134e61ff65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error on train and validation:  0.00036709356405661975\n",
            "Error on test data:  0.31311736735448314\n"
          ]
        }
      ],
      "source": [
        "neigh_dist = KNeighborsRegressor(weights='distance')\n",
        "\n",
        "#fit the model on training and validation scaled data\n",
        "neigh_dist.fit(Xtrain_and_val_scaled, Ytrain_and_val)\n",
        "\n",
        "print(\"Error on train and validation: \",1. - neigh_dist.score(Xtrain_and_val_scaled,Ytrain_and_val))\n",
        "print(\"Error on test data: \",1. - neigh_dist.score(Xtest_scaled,Ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D71XKG2WCJwd"
      },
      "source": [
        "## TO DO: use cross validation to choose the best number of neighbours between 2 and 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDoXQwfICJwd",
        "outputId": "30d6b25c-ad81-4df4-9098-7263675b5960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:    9.2s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=KNeighborsRegressor(),\n",
              "             param_grid={'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
              "                                         14, 15, 16, 17, 18, 19],\n",
              "                         'weights': ['uniform', 'distance']},\n",
              "             verbose=True)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "neigh_cv = KNeighborsRegressor()\n",
        "param_grid = {'n_neighbors': [i for i in range(2,20)],\n",
        "              'weights':['uniform','distance']\n",
        "             }\n",
        "neigh_GS = GridSearchCV( neigh_cv, param_grid=param_grid, cv=5, verbose=True)\n",
        "neigh_GS.fit(Xtrain_and_val_scaled, Ytrain_and_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMIBGiw2CJwe"
      },
      "source": [
        "## TO DO: print the best model according to cross validation above, and print the score of the best model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ0wr7EQCJwe",
        "outputId": "5d9387fa-900a-4492-f7dd-7348490463bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model:  KNeighborsRegressor(n_neighbors=6, weights='distance')\n",
            "Score of best model:  0.7917623409789576\n"
          ]
        }
      ],
      "source": [
        "#let's print the best model according to grid search\n",
        "print(\"Best model: \",neigh_GS.best_estimator_)\n",
        "#let's print the score for the best model\n",
        "print(\"Score of best model: \", neigh_GS.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrfu0HxyCJwe"
      },
      "source": [
        "## TO DO: learn the best model on all of the training and validation scaled data, and print the error on training and validation scaled data, and on test scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdzPsosPCJwe",
        "outputId": "cf66c05b-2f4f-4b50-8fc0-9d2b8d25b417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error best model on train and validation:  0.00036709356405661975\n",
            "Error best model on test data:  0.3216866455997077\n"
          ]
        }
      ],
      "source": [
        "best_neigh = KNeighborsRegressor(n_neighbors=6, weights='distance')\n",
        "best_neigh.fit(Xtrain_and_val_scaled,Ytrain_and_val)\n",
        "\n",
        "print(\"Error best model on train and validation: \",1. - best_neigh.score(Xtrain_and_val_scaled,Ytrain_and_val))\n",
        "print(\"Error best model on test data: \",1. - best_neigh.score(Xtest_scaled,Ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeCccWhaCJwf"
      },
      "source": [
        "## TO DO: compare the error on test data of the best kNN model with the error on test data of linear regression and of NNs. Describe what you observe and give a potential explanation.\n",
        "## [USE MAX 10 LINES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_qKcioECJwf"
      },
      "source": [
        "Error best model on test data\n",
        "\n",
        "kNN: 0.3216866455997077\n",
        "\n",
        "Linear Regression:0.326693824565925\n",
        "\n",
        "NN: 0.18499202712189133\n",
        "\n",
        "The best model of NN performs better than kNN and Linear Regression. \n",
        "The Neural network outperforms linear regression probably beacuse let us to define non linear functions.\n",
        "In this case looking at the big difference of R^2 between NN and Linear Model we can say that we deal with non linearities.\n",
        "Comparing kNN (a non-parametric method) to NN and Linear Model (parametric methods) we can see that\n",
        "kNN overfits on training and validation set and can't generalize in a good way on test set.\n",
        "This could be also a problem caused by a small number of samples on Training and Validation data or their particular distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5QeaU8DCJwf"
      },
      "source": [
        "# Clustering and \"Local\" Linear Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTIHsdXYCJwg"
      },
      "source": [
        "You are now going to explore the use of clustering to identify groups of *similar* instances, and then learning models that are specific to each group.\n",
        "\n",
        "Once you have clustered the data, and then learned a model for each cluster, the prediction for a new instance is obtained by using the model of the cluster that is the closest to the instance, where the distance of a cluster to the instance is defined as the distance of the *center* of the cluster to the instance.\n",
        "\n",
        "**Note**: in this part you are not explicitely told which part of the data to use, deciding which one is the correct one is part of the homework!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEQOXzS9CJwg"
      },
      "source": [
        "## TO DO: use k-means in sklearn to learn a cluster with 5 clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-lVfMneCJwg",
        "outputId": "d712c2a9-3708-4bfd-92e7-bbb5b9eb2bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN AND VAL:\n",
            "label:  [0 1 2 3 4] \n",
            "counter:  [270 863 735  36 732]\n",
            "TEST:\n",
            "label:  [0 1 2 3 4] \n",
            "counter:  [ 52 172 144  11 149]\n"
          ]
        }
      ],
      "source": [
        "#load the required packages\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=numero_di_matricola).fit(Xtrain_and_val_scaled)\n",
        "Ytrain_and_val_predicted=kmeans.predict(Xtrain_and_val_scaled)\n",
        "Ytest_predicted=kmeans.predict(Xtest_scaled)\n",
        "\n",
        "#I print the number of samples\n",
        "label,counter=np.unique(Ytrain_and_val_predicted, return_counts = True)\n",
        "print(\"TRAIN AND VAL:\\nlabel: \",label,\"\\ncounter: \",counter)\n",
        "label,counter=np.unique(Ytest_predicted, return_counts = True)\n",
        "print(\"TEST:\\nlabel: \",label,\"\\ncounter: \",counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-MPd5naCJwg"
      },
      "source": [
        "## TO DO: for each cluster, learn a linear model using the elements of the cluster. For each model, print the error on the data used to learn it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3Q7RxMuCJwh",
        "outputId": "6749befb-04f6-4c08-9c85-7e16bbc045f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster : 0   Error on train and validation:  0.23703594108805937\n",
            "Cluster : 1   Error on train and validation:  0.35052480765457983\n",
            "Cluster : 2   Error on train and validation:  0.34723044510453116\n",
            "Cluster : 3   Error on train and validation:  0.03503035380115194\n",
            "Cluster : 4   Error on train and validation:  0.3431290290135154\n"
          ]
        }
      ],
      "source": [
        "#LRcluster array of the linear regression models\n",
        "LRcluster=[]\n",
        "for i in range(0,5):\n",
        "    #let's construct the training and validation set of cluster i\n",
        "    X_train_cluster=Xtrain_and_val_scaled[np.where(Ytrain_and_val_predicted==i)]\n",
        "    Y_train_cluster=Ytrain_and_val[np.where(Ytrain_and_val_predicted==i)]\n",
        "    #LRi linear regression model of cluster i\n",
        "    LRi = linear_model.LinearRegression()\n",
        "    #fit the model on training data of cluster i\n",
        "    LRi.fit(X_train_cluster, Y_train_cluster)\n",
        "    print(\"Cluster :\",i, \"  Error on train and validation: \",1. - LRi.score(X_train_cluster,Y_train_cluster))\n",
        "    LRcluster.append(LRi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrZD7nOkCJwh"
      },
      "source": [
        "## TO DO: *compute* the error (1 - R^2) on the data not used to learn the models.\n",
        "For each instance not used to learn the model, the prediction is done by:\n",
        "- finding the cluster C whose center is the closest to the instance\n",
        "- use the model learned for cluster C to make the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4WrJnAkCJwh"
      },
      "outputs": [],
      "source": [
        "#Y_predicted_LR contains the prediction of the model on test data\n",
        "Y_predicted_LR=np.array([])\n",
        "#Y_test_LR will be Ytest reordered to let me calculate R^2\n",
        "Y_test_LR=np.array([])\n",
        "\n",
        "for i in range(0,5):\n",
        "    #let's construct the test set of cluster i\n",
        "    X_test_cluster=Xtest_scaled[np.where(Ytest_predicted==i)]\n",
        "    Y_test_cluster=Ytest[np.where(Ytest_predicted==i)]\n",
        "    #predictions of the model on test data of cluster i\n",
        "    Y_predicted_LR=np.append(Y_predicted_LR,LRcluster[i].predict(X_test_cluster))\n",
        "    Y_test_LR=np.append(Y_test_LR,Y_test_cluster)\n",
        "\n",
        "#calculate R^2 using the function r2_score\n",
        "measure_test=r2_score(Y_test_LR, Y_predicted_LR)\n",
        "#calculate R^2 esplicity\n",
        "#measure_test = 1-np.linalg.norm(Y_test_LR-Y_predicted_LR)**2/np.linalg.norm(Y_test_LR-Y_test_LR.mean())**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF3aE8HtCJwi"
      },
      "source": [
        "## TO DO: *print* the error (1-R^2) on the data not used to learn the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by0J4p_oCJwi",
        "outputId": "633994c9-b9ed-4794-baa9-b969ab758cfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error on Test Data (1-R^2): 0.28057902375638644\n"
          ]
        }
      ],
      "source": [
        "print(\"Error on Test Data (1-R^2):\", 1-measure_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcbgBFG8CJwi"
      },
      "source": [
        "## TO DO: compare the error of the model \"clustering + linear models\" and of the linear model (see the beginning of the HW). Describe what you observe, and provide a possible explanation.\n",
        "## [USE MAX 10 LINES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFRRUAx_CJwj"
      },
      "source": [
        "Values of 1-RÂ² for models:\n",
        "\n",
        "CLUSTERING + LINEAR MODELS:   0.2805790237563843\n",
        "\n",
        "\n",
        "BEST LINEAR MODEL:    0.326693824565925\n",
        "\n",
        "\"clustering + linear modles\" performs better than linear model.\n",
        "A possible explenation is that using clustering we can \"groupped similar samples\" in such a way that\n",
        "the distribution of the samples that belong to a specific cluster is a \"more linear\" function compared to the one that a linear model try to learn from the entire training and validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoez8OjMCJwj"
      },
      "source": [
        "## TO DO: compare the error of the model \"clustering + linear models\" and of kNN. Describe what you observe, and provide a possible explanation.\n",
        "## [USE MAX 10 LINES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msnHALuaCJwj"
      },
      "source": [
        "CLUSTERING + LINEAR MODELS: \n",
        "\n",
        "-Test Error (1-R^2):    0.2805790237563843\n",
        "\n",
        "BEST kNN:   \n",
        "\n",
        "-Test Error (1-R^2):    0.3216866455997077\n",
        "\n",
        "As we can see \"Cluster+linear models\" performs better than kNN.\n",
        "\n",
        "kNN seems to overfit on training set and can't generalize well on test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Po2iddOCJwj"
      },
      "source": [
        "# Clustering and \"Local\" NNs\n",
        "\n",
        "Repeat the same as above, but using neural networks instead of linear models.\n",
        "\n",
        "**Note**: note that we are not telling you which parameters to use for NNs. You have to decide how to select the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt7uwxoKCJwk"
      },
      "source": [
        "## TO DO: clearly explain how you decided to set the parameters, motivating the choice of your strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRB9pgYcCJwk"
      },
      "source": [
        "I decide to use cross validation to find the best NN architecture for each cluster.\n",
        "As before I tried different sizes for the hidden layer.\n",
        "I decide to not use more than one hidden layer to simplify the running of the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VP76P6oCJwk"
      },
      "source": [
        "## TO DO: repeat the analysis in part \"Clustering and \"Local\" Linear Models\" using NNs instead of linear models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2LwC4R0CJwk",
        "outputId": "adac893e-2b38-4e1a-a718-0e9e2001b186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster : 0    Samples:  270\n",
            "Best model:  MLPRegressor(hidden_layer_sizes=3, max_iter=3000, random_state=2019157,\n",
            "             solver='lbfgs')\n",
            "Cluster : 1    Samples:  863\n",
            "Best model:  MLPRegressor(hidden_layer_sizes=6, max_iter=3000, random_state=2019157,\n",
            "             solver='lbfgs')\n",
            "Cluster : 2    Samples:  735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\scatt\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model:  MLPRegressor(hidden_layer_sizes=8, max_iter=3000, random_state=2019157,\n",
            "             solver='lbfgs')\n",
            "Cluster : 3    Samples:  36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\scatt\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model:  MLPRegressor(hidden_layer_sizes=7, max_iter=3000, random_state=2019157,\n",
            "             solver='lbfgs')\n",
            "Cluster : 4    Samples:  732\n",
            "Best model:  MLPRegressor(hidden_layer_sizes=8, max_iter=3000, random_state=2019157,\n",
            "             solver='lbfgs')\n"
          ]
        }
      ],
      "source": [
        "#I divide the code in 3 pieces to simplify the running and checking during changes on the code\n",
        "NNcluster=[]\n",
        "for i in range(0,5):\n",
        "    #let's construct the training and validation set of cluster i\n",
        "    X_train_val_cluster=Xtrain_and_val_scaled[np.where(Ytrain_and_val_predicted==i)]\n",
        "    Y_train_val_cluster=Ytrain_and_val[np.where(Ytrain_and_val_predicted==i)]\n",
        "    print('Cluster :',i, '   Samples: ',X_train_val_cluster.shape[0])\n",
        "    mlp_cv_c = MLPRegressor(max_iter=3000)\n",
        "    param_grid = {'hidden_layer_sizes':[i for i in range(1,10)],\n",
        "                'activation': ['relu'],\n",
        "              'solver': ['lbfgs'], \n",
        "              'random_state': [numero_di_matricola],\n",
        "             }\n",
        "    mlp_GS_c = GridSearchCV( mlp_cv_c, param_grid=param_grid, cv=5)\n",
        "    mlp_GS_c.fit(X_train_val_cluster, Y_train_val_cluster)\n",
        "    #let's print the best model according to grid search\n",
        "    print(\"Best model: \",mlp_GS_c.best_estimator_)\n",
        "    NNcluster.append(mlp_GS_c.best_estimator_)\n",
        "    #let's fit the best model on training and validation set of cluster i\n",
        "    NNcluster[i].fit(X_train_val_cluster, Y_train_val_cluster)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc84BUEnCJwl"
      },
      "outputs": [],
      "source": [
        "#Y_predicted_NN contains the prediction of the model on test data\n",
        "Y_predicted_NN=np.array([])\n",
        "#Y_test_NN will be Ytest reordered to let me calculate R^2\n",
        "Y_test_NN=np.array([])\n",
        "\n",
        "for i in range(0,5):\n",
        "    #let's construct the test set of cluster i\n",
        "    X_test_cluster_nn=Xtest_scaled[np.where(Ytest_predicted==i)]\n",
        "    Y_test_cluster_nn=Ytest[np.where(Ytest_predicted==i)]\n",
        "    #predictions of the model on test data of cluster i\n",
        "    Y_predicted_NN=np.append(Y_predicted_NN,NNcluster[i].predict(X_test_cluster_nn))\n",
        "    Y_test_NN=np.append(Y_test_NN,Y_test_cluster_nn)\n",
        "    \n",
        "#calculate R^2 using the function r2_score\n",
        "measure_test_NN=r2_score(Y_test_NN, Y_predicted_NN)\n",
        "#calculate R^2 esplicity\n",
        "#measure_test_NN = 1-np.linalg.norm(Y_test_NN-Y_predicted_NN)**2/np.linalg.norm(Y_test_NN-Y_test_NN.mean())**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4zJGZB3CJwl",
        "outputId": "abf27126-a0e1-4c64-8246-94d9d7d92c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error on Test Data(1-R^2): 0.19603322161505854\n"
          ]
        }
      ],
      "source": [
        "print(\"Error on Test Data(1-R^2):\", 1-measure_test_NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK9CI0UBCJwl"
      },
      "source": [
        "## TO DO: compare the error of the model \"clustering + NNs\" and of NNs (see the beginning of the HW). Describe what you observe, and provide a possible explanation.\n",
        "## [USE MAX 10 LINES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbc-T-CzCJwl"
      },
      "source": [
        "CLUSTERING + NNs\n",
        "\n",
        "Test Error (1-R^2) :  0.19603322161505843\n",
        "\n",
        "\n",
        "BEST NN\n",
        "Test Error (1-R^2) :  0.18499202712189133\n",
        "\n",
        "We can see that using a single NN trained on all the training data performs better than the combination of cluster with NNs.\n",
        "The possible explenation is that some clusters contains little data for Training (for examples cluster 3 for training set contains 36 samples and in the test 11 samples) so the NN trained on that cluster can't set in a good way its parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSNL-73ECJwm"
      },
      "source": [
        "## TO DO: compare the error of the model \"clustering + NNs\" and of kNN. Describe what you observe, and provide a possible explanation.\n",
        "## [USE MAX 10 LINES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mMd6KWJCJwm"
      },
      "source": [
        "CLUSTERING + NNs\n",
        "\n",
        "Test Error (1-R^2) :  0.19603322161505843\n",
        "\n",
        "BEST kNN\n",
        "\n",
        "-Test Error (1-R^2):    0.3216866455997077\n",
        "\n",
        "Clustering+NNs performs better than kNN.\n",
        "kNN overfits on training and validation set and can't generalize in a good way on test set.\n",
        "This could be also a problem caused by a small number of samples on Training and Validation data or their particular distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfc-cOXCCJwm"
      },
      "source": [
        "## TO DO: compare the error of the model \"clustering + NNs\" and of \"clustering + Linear Models\". Describe what you observe, and provide a possible explanation.\n",
        "## [USE MAX 10 LINES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sLz_EdOCJwm"
      },
      "source": [
        "CLUSTERING + NNs\n",
        "\n",
        "-Test Error (1-R^2):    0.2805790237563843\n",
        "\n",
        "CLUSTERING + LINEAR MODELS: \n",
        "\n",
        "Test Error (1-R^2) :  0.19603322161505843\n",
        "\n",
        "\n",
        "For the same reason as before when we compared NNs with Linear models we can say that \"clustering + NNs\" outperform \"clustering + Linear models\" beacuse probably we are dealing with non linearities.\n",
        "So the single NN model used for one cluster performs better than the Linear model used on the same cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONJbmkEgCJwm"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "4_NN_kNN_Clustering.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}